#!/usr/bin/env python3
"""
ChangeLog.mdè§£æãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¿…è¦ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ã™ã‚‹æ±ç”¨çš„ãªè§£æãƒ„ãƒ¼ãƒ«

é…ç½®å ´æ‰€: Agent-shared/tools/changelog_analyzer.py
å‡ºåŠ›å…ˆ: Agent-shared/reports/ (æŠ€è¡“çš„ãªè§£æçµæœ)

æ³¨æ„: ã“ã‚Œã¯ä¸€æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ(ChangeLog.md)ã‚’è§£æã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
äºŒæ¬¡ãƒ¬ãƒãƒ¼ãƒˆ(User-shared/reports/)ã¯SEãŒæ‰‹å‹•ã§ä½œæˆã—ã¾ã™ã€‚
"""

import os
import re
import json
from datetime import datetime, timezone
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional, Any

class ChangeLogAnalysisTemplate:
    """
    æ±ç”¨çš„ãªChangeLog.mdè§£æã‚¯ãƒ©ã‚¹
    SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç¶™æ‰¿ãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æƒ³å®š
    
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯æŠ€è¡“çš„ãªè§£æã‚’è¡Œã†ãŸã‚ã®ã‚‚ã®ã§ã€
    äººé–“å‘ã‘ã®äºŒæ¬¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã¯åˆ¥é€”æ‰‹å‹•ã§è¡Œã„ã¾ã™ã€‚
    """
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.reports_dir = self.project_root / "Agent-shared" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
    def find_target_files(self, filename: str = "ChangeLog.md", 
                         exclude_dirs: List[str] = ["Agent-shared", "GitHub", "BaseCode"]) -> List[Path]:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        
        Args:
            filename: æ¤œç´¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
            exclude_dirs: é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
        """
        target_files = []
        for root, dirs, files in os.walk(self.project_root):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            if any(skip in root for skip in exclude_dirs):
                continue
            if filename in files:
                target_files.append(Path(root) / filename)
        return target_files
    
    def parse_entry(self, content: str) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
        ChangeLog.mdã®æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
        """
        entries = []
        
        # ### v1.2.3 ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¨ãƒ³ãƒˆãƒªã‚’åˆ†å‰²
        version_pattern = r'###\s+v([\d.]+)'
        
        # ã‚¨ãƒ³ãƒˆãƒªã”ã¨ã«åˆ†å‰²
        matches = list(re.finditer(version_pattern, content))
        
        for i, match in enumerate(matches):
            version = f"v{match.group(1)}"
            start = match.start()
            end = matches[i+1].start() if i+1 < len(matches) else len(content)
            entry_content = content[start:end]
            
            entry = {"version": version}
            
            # æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
            # å¤‰æ›´ç‚¹ã€çµæœã€ã‚³ãƒ¡ãƒ³ãƒˆ
            change_match = re.search(r'\*\*å¤‰æ›´ç‚¹\*\*:\s*"([^"]+)"', entry_content)
            if change_match:
                entry["change_summary"] = change_match.group(1)
            
            result_match = re.search(r'\*\*çµæœ\*\*:\s*([^`]+)\s*`([^`]+)`', entry_content)
            if result_match:
                entry["result_type"] = result_match.group(1).strip()
                entry["result_value"] = result_match.group(2).strip()
            
            comment_match = re.search(r'\*\*ã‚³ãƒ¡ãƒ³ãƒˆ\*\*:\s*"([^"]+)"', entry_content)
            if comment_match:
                entry["technical_comment"] = comment_match.group(1)
            
            # <details>å†…ã®æƒ…å ±ã‚’ãƒ‘ãƒ¼ã‚¹
            details_match = re.search(r'<details>([\s\S]*?)</details>', entry_content)
            if details_match:
                details_content = details_match.group(1)
                
                # compileæƒ…å ±
                compile_match = re.search(r'-\s*\[([x\s])\]\s*\*\*compile\*\*[\s\S]*?status:\s*`([^`]+)`', details_content)
                if compile_match:
                    entry["compile_complete"] = compile_match.group(1) == 'x'
                    entry["compile_status"] = compile_match.group(2)
                
                # jobæƒ…å ±
                job_match = re.search(r'-\s*\[([x\s])\]\s*\*\*job\*\*[\s\S]*?status:\s*`([^`]+)`', details_content)
                if job_match:
                    entry["job_complete"] = job_match.group(1) == 'x'
                    entry["job_status"] = job_match.group(2)
                
                # testæƒ…å ±
                test_match = re.search(r'-\s*\[([x\s])\]\s*\*\*test\*\*[\s\S]*?status:\s*`([^`]+)`', details_content)
                if test_match:
                    entry["test_complete"] = test_match.group(1) == 'x'
                    entry["test_status"] = test_match.group(2)
                
                # performance
                perf_match = re.search(r'performance:\s*`([^`]+)`', details_content)
                if perf_match:
                    entry["performance"] = perf_match.group(1)
                
                # sota
                sota_match = re.search(r'-\s*\[x\]\s*\*\*sota\*\*[\s\S]*?scope:\s*`([^`]+)`', details_content)
                if sota_match:
                    entry["sota_scope"] = sota_match.group(1)
            
            entries.append(entry)
        
        return entries
    
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Returns:
            æŠ½å‡ºã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        parts = file_path.parts
        metadata = {
            "file_path": str(file_path),
            "directory_path": str(file_path.parent),
            "path_components": list(parts),
        }
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã®æŠ½å‡ºï¼ˆPG, SEç­‰ï¼‰
        for part in parts:
            if re.match(r'(PG|SE|CD|PM)\d*(\.\d+)*', part):
                metadata["agent"] = part
                break
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º
        # SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        
        return metadata
    
    def aggregate_data(self, all_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Args:
            all_data: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã€ã‚¨ãƒ³ãƒˆãƒªãƒªã‚¹ãƒˆã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
            
        Returns:
            é›†è¨ˆçµæœã®è¾æ›¸
        """
        stats = {
            "total_entries": 0,
            "by_status": defaultdict(int),
            "by_agent": defaultdict(lambda: {"total": 0, "success": 0}),
            "sota_updates": defaultdict(int),
            "timeline": []
        }
        
        for file_path, entries in all_data.items():
            for entry in entries:
                stats["total_entries"] += 1
                
                # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥é›†è¨ˆ
                compile_status = entry.get("compile_status", "unknown")
                stats["by_status"][compile_status] += 1
                
                # SOTAæ›´æ–°ã®é›†è¨ˆ
                sota_scope = entry.get("sota_scope")
                if sota_scope:
                    stats["sota_updates"][sota_scope] += 1
                
                # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”¨ãƒ‡ãƒ¼ã‚¿
                if "timestamp" in entry:
                    stats["timeline"].append({
                        "timestamp": entry["timestamp"],
                        "version": entry.get("version", "unknown"),
                        "status": compile_status,
                        "file": str(file_path)
                    })
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ã‚½ãƒ¼ãƒˆ
        stats["timeline"].sort(key=lambda x: x["timestamp"])
        
        return stats
    
    def generate_report(self, stats: Dict[str, Any], report_type: str = "summary") -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Args:
            stats: é›†è¨ˆãƒ‡ãƒ¼ã‚¿
            report_type: ãƒ¬ãƒãƒ¼ãƒˆã®ç¨®é¡
        """
        now = datetime.now(timezone.utc)
        report = f"# ChangeLog Report - {report_type.title()}\n\n"
        report += f"Generated at: {now.strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
        
        # åŸºæœ¬çµ±è¨ˆ
        report += "## ğŸ“Š Summary\n\n"
        report += f"- Total entries: {stats['total_entries']}\n"
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥
        report += "\n### Status Breakdown\n"
        for status, count in stats['by_status'].items():
            percentage = (count / stats['total_entries'] * 100) if stats['total_entries'] > 0 else 0
            report += f"- {status}: {count} ({percentage:.1f}%)\n"
        
        # SOTAæ›´æ–°
        if stats['sota_updates']:
            report += "\n### SOTA Updates\n"
            for level, count in stats['sota_updates'].items():
                report += f"- {level}: {count}\n"
        
        return report
    
    def run(self, custom_params: Dict[str, Any] = None):
        """
        ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ
        
        Args:
            custom_params: ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        params = custom_params or {}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        target_files = self.find_target_files(
            filename=params.get("filename", "ChangeLog.md"),
            exclude_dirs=params.get("exclude_dirs", ["Agent-shared", "GitHub", "BaseCode"])
        )
        
        print(f"Found {len(target_files)} target files")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        all_data = {}
        for file_path in target_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                entries = self.parse_entry(content)
                if entries:
                    all_data[str(file_path)] = entries
                    print(f"âœ“ Processed: {file_path} ({len(entries)} entries)")
                    
            except Exception as e:
                print(f"âœ— Error processing {file_path}: {e}")
        
        # é›†è¨ˆ
        stats = self.aggregate_data(all_data)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_report(stats)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        now = datetime.now(timezone.utc)
        report_path = self.reports_dir / f"report_{now.strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ Analysis report saved to: {report_path}")
        print(f"ğŸ’¡ Note: This is a technical analysis. For user-facing reports, create manually in User-shared/reports/")
        return report_path


# ä½¿ç”¨ä¾‹ï¼ˆSEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ï¼‰
class HPCOptimizationAnalysis(ChangeLogAnalysisTemplate):
    """HPCæœ€é©åŒ–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã®è§£æã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹"""
    
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        metadata = super().extract_metadata(file_path)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æŠ€è¡“ã‚’å‹•çš„ã«æŠ½å‡º
        parts = file_path.parts
        technologies = []
        
        for part in parts:
            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢åŒºåˆ‡ã‚Šã®æŠ€è¡“åã‚’åˆ†è§£
            if "_" in part:
                potential_techs = part.split("_")
                technologies.extend(potential_techs)
            else:
                technologies.append(part)
        
        # ã‚ˆãçŸ¥ã‚‰ã‚ŒãŸæŠ€è¡“åã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
        known_techs = ["OpenMP", "MPI", "CUDA", "OpenACC", "AVX", "AVX2", "AVX512"]
        found_techs = [t for t in technologies if any(k in t for k in known_techs)]
        
        if found_techs:
            metadata["technologies"] = found_techs
        
        return metadata


if __name__ == "__main__":
    # åŸºæœ¬çš„ãªä½¿ç”¨
    analyzer = ChangeLogAnalysisTemplate()
    analyzer.run()
    
    # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸä½¿ç”¨
    # hpc_analyzer = HPCOptimizationReport()
    # hpc_analyzer.run()
    
    # æ³¨: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Agent-shared/tools/ã«é…ç½®ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨