#!/usr/bin/env python3
"""
changes.mdãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¿…è¦ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ã™ã‚‹æ±ç”¨çš„ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«
"""

import os
import re
import json
from datetime import datetime, timezone
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional, Any

class ChangesReportTemplate:
    """
    æ±ç”¨çš„ãªchanges.mdè§£æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹
    SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç¶™æ‰¿ãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æƒ³å®š
    """
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.reports_dir = self.project_root / "Agent-shared" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
    def find_target_files(self, filename: str = "changes.md", 
                         exclude_dirs: List[str] = ["Agent-shared", "GitHub", "BaseCode"]) -> List[Path]:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        
        Args:
            filename: æ¤œç´¢ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
            exclude_dirs: é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
        """
        target_files = []
        for root, dirs, files in os.walk(self.project_root):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            if any(skip in root for skip in exclude_dirs):
                continue
            if filename in files:
                target_files.append(Path(root) / filename)
        return target_files
    
    def parse_entry(self, content: str) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
        ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯changes.mdã®æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
        """
        entries = []
        # version: v1.2.3 ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¨ãƒ³ãƒˆãƒªã‚’åˆ†å‰²
        version_pattern = r'(?:^|\n)(?:##?\s*)?version:\s*(v[\d.]+)'
        
        # ã‚¨ãƒ³ãƒˆãƒªã”ã¨ã«åˆ†å‰²
        splits = re.split(version_pattern, content)
        
        for i in range(1, len(splits), 2):
            if i+1 < len(splits):
                version = splits[i]
                entry_content = splits[i+1]
                
                entry = {"version": version}
                
                # æ¨™æº–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æŠ½å‡ºï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ãƒ»å¤‰æ›´å¯èƒ½ï¼‰
                patterns = {
                    "change_summary": r'change_summary:\s*"([^"]*)"',
                    "timestamp": r'timestamp:\s*"([^"]*)"',
                    "compile_status": r'compile_status:\s*(\w+)',
                    "job_status": r'job_status:\s*(\w+)',
                    "performance_metric": r'performance_metric:\s*"([^"]*)"',
                    "sota_level": r'sota_level:\s*(\w+)',
                    "technical_comment": r'technical_comment:\s*"([^"]*)"'
                }
                
                for field, pattern in patterns.items():
                    match = re.search(pattern, entry_content)
                    if match:
                        entry[field] = match.group(1)
                
                entries.append(entry)
        
        return entries
    
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Returns:
            æŠ½å‡ºã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        parts = file_path.parts
        metadata = {
            "file_path": str(file_path),
            "directory_path": str(file_path.parent),
            "path_components": list(parts),
        }
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã®æŠ½å‡ºï¼ˆPG, CI, SEç­‰ï¼‰
        for part in parts:
            if re.match(r'(PG|CI|SE|CD|ID|PM)\d*(\.\d+)*', part):
                metadata["agent"] = part
                break
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º
        # SEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        
        return metadata
    
    def aggregate_data(self, all_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Args:
            all_data: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã€ã‚¨ãƒ³ãƒˆãƒªãƒªã‚¹ãƒˆã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
            
        Returns:
            é›†è¨ˆçµæœã®è¾æ›¸
        """
        stats = {
            "total_entries": 0,
            "by_status": defaultdict(int),
            "by_agent": defaultdict(lambda: {"total": 0, "success": 0}),
            "sota_updates": defaultdict(int),
            "timeline": []
        }
        
        for file_path, entries in all_data.items():
            for entry in entries:
                stats["total_entries"] += 1
                
                # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥é›†è¨ˆ
                compile_status = entry.get("compile_status", "unknown")
                stats["by_status"][compile_status] += 1
                
                # SOTAæ›´æ–°ã®é›†è¨ˆ
                sota_level = entry.get("sota_level", "none")
                if sota_level != "none":
                    stats["sota_updates"][sota_level] += 1
                
                # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç”¨ãƒ‡ãƒ¼ã‚¿
                if "timestamp" in entry:
                    stats["timeline"].append({
                        "timestamp": entry["timestamp"],
                        "version": entry.get("version", "unknown"),
                        "status": compile_status,
                        "file": str(file_path)
                    })
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ã‚½ãƒ¼ãƒˆ
        stats["timeline"].sort(key=lambda x: x["timestamp"])
        
        return stats
    
    def generate_report(self, stats: Dict[str, Any], report_type: str = "summary") -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¨å¥¨ï¼‰
        
        Args:
            stats: é›†è¨ˆãƒ‡ãƒ¼ã‚¿
            report_type: ãƒ¬ãƒãƒ¼ãƒˆã®ç¨®é¡
        """
        now = datetime.now(timezone.utc)
        report = f"# Changes Report - {report_type.title()}\n\n"
        report += f"Generated at: {now.strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
        
        # åŸºæœ¬çµ±è¨ˆ
        report += "## ğŸ“Š Summary\n\n"
        report += f"- Total entries: {stats['total_entries']}\n"
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥
        report += "\n### Status Breakdown\n"
        for status, count in stats['by_status'].items():
            percentage = (count / stats['total_entries'] * 100) if stats['total_entries'] > 0 else 0
            report += f"- {status}: {count} ({percentage:.1f}%)\n"
        
        # SOTAæ›´æ–°
        if stats['sota_updates']:
            report += "\n### SOTA Updates\n"
            for level, count in stats['sota_updates'].items():
                report += f"- {level}: {count}\n"
        
        return report
    
    def run(self, custom_params: Dict[str, Any] = None):
        """
        ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ
        
        Args:
            custom_params: ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        params = custom_params or {}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        target_files = self.find_target_files(
            filename=params.get("filename", "changes.md"),
            exclude_dirs=params.get("exclude_dirs", ["Agent-shared", "GitHub", "BaseCode"])
        )
        
        print(f"Found {len(target_files)} target files")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        all_data = {}
        for file_path in target_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                entries = self.parse_entry(content)
                if entries:
                    all_data[str(file_path)] = entries
                    print(f"âœ“ Processed: {file_path} ({len(entries)} entries)")
                    
            except Exception as e:
                print(f"âœ— Error processing {file_path}: {e}")
        
        # é›†è¨ˆ
        stats = self.aggregate_data(all_data)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_report(stats)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        now = datetime.now(timezone.utc)
        report_path = self.reports_dir / f"report_{now.strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ Report saved to: {report_path}")
        return report_path


# ä½¿ç”¨ä¾‹ï¼ˆSEã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ï¼‰
class HPCOptimizationReport(ChangesReportTemplate):
    """HPCæœ€é©åŒ–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹"""
    
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        metadata = super().extract_metadata(file_path)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æŠ€è¡“ã‚’å‹•çš„ã«æŠ½å‡º
        parts = file_path.parts
        technologies = []
        
        for part in parts:
            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢åŒºåˆ‡ã‚Šã®æŠ€è¡“åã‚’åˆ†è§£
            if "_" in part:
                potential_techs = part.split("_")
                technologies.extend(potential_techs)
            else:
                technologies.append(part)
        
        # ã‚ˆãçŸ¥ã‚‰ã‚ŒãŸæŠ€è¡“åã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
        known_techs = ["OpenMP", "MPI", "CUDA", "OpenACC", "AVX", "AVX2", "AVX512"]
        found_techs = [t for t in technologies if any(k in t for k in known_techs)]
        
        if found_techs:
            metadata["technologies"] = found_techs
        
        return metadata


if __name__ == "__main__":
    # åŸºæœ¬çš„ãªä½¿ç”¨
    reporter = ChangesReportTemplate()
    reporter.run()
    
    # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸä½¿ç”¨
    # hpc_reporter = HPCOptimizationReport()
    # hpc_reporter.run()