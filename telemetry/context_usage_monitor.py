#!/usr/bin/env python3

"""
VibeCodeHPC ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨ç‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
Claude Code JSONLãƒ­ã‚°ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨çŠ¶æ³ã‚’è§£æã—ã€å„ç¨®ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–

æ©Ÿèƒ½:
1. agent_and_pane_id_table.jsonlã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å‹•çš„å–å¾—
2. ~/.claude/projects/ ä»¥ä¸‹ã®JSONLãƒ­ã‚°ã‚’ç›£è¦–
3. usageæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
4. å¤šæ§˜ãªã‚°ãƒ©ãƒ•å½¢å¼ã§å¯è¦–åŒ–ï¼ˆç©ã¿ä¸Šã’æ£’ã€æŠ˜ã‚Œç·šã€æ¦‚è¦ï¼‰
5. auto-compactï¼ˆ160Kå‰å¾Œï¼‰ã®äºˆæ¸¬
6. è»½é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
7. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªæ©Ÿèƒ½
8. æ™‚é–“åˆ¶é™ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ--max-minutesï¼‰ã§ã‚°ãƒ©ãƒ•ã®è¡¨ç¤ºç¯„å›²ã‚’åˆ¶å¾¡
"""

import json
import os
import sys
import subprocess
import platform
from pathlib import Path
from datetime import datetime, timezone, timedelta
import argparse
import matplotlib
matplotlib.use('Agg')  # GUIãªã—ç’°å¢ƒã§ã‚‚å‹•ä½œ
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import defaultdict, OrderedDict
from typing import Dict, List, Tuple, Optional
import numpy as np
import pickle
import gzip

# ã‚°ãƒ©ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
try:
    plt.style.use('seaborn-v0_8-darkgrid')
except:
    plt.style.use('seaborn-darkgrid')
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 10

class ContextUsageMonitor:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨ç‡ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    # Claude Codeã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™
    CONTEXT_LIMIT = 200000  # 200Kãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¡¨ç¤ºç”¨ï¼‰
    AUTO_COMPACT_THRESHOLD = 160000  # å®Ÿéš›ã®auto-compactç™ºç”Ÿç‚¹ï¼ˆæ¨å®šï¼‰
    WARNING_THRESHOLD = 140000  # è­¦å‘Šé–¾å€¤
    
    def __init__(self, project_root: Path, use_cache: bool = True, max_minutes: Optional[int] = None):
        self.project_root = project_root
        self.claude_projects_dir = self._get_claude_projects_dir()
        self.output_dir = project_root / "User-shared" / "visualizations"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_minutes = max_minutes  # æ™‚é–“åˆ¶é™ï¼ˆåˆ†ï¼‰
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.use_cache = use_cache
        self.cache_dir = project_root / ".cache" / "context_monitor"
        if self.use_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_claude_projects_dir(self) -> Path:
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«å¿œã˜ãŸClaude projectsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
        return Path.home() / ".claude" / "projects"
    
    def get_cache_path(self, agent_id: str, jsonl_file: Path) -> Path:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
        cache_name = f"{agent_id}_{jsonl_file.stem}.pkl.gz"
        return self.cache_dir / cache_name
    
    def load_from_cache(self, cache_path: Path, jsonl_file: Path) -> Optional[List[Dict]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if not self.use_cache or not cache_path.exists():
            return None
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’æ¯”è¼ƒ
        cache_mtime = cache_path.stat().st_mtime
        jsonl_mtime = jsonl_file.stat().st_mtime
        
        if jsonl_mtime > cache_mtime:
            return None  # JSONLã®æ–¹ãŒæ–°ã—ã„
            
        try:
            with gzip.open(cache_path, 'rb') as f:
                return pickle.load(f)
        except:
            return None
    
    def save_to_cache(self, cache_path: Path, data: List[Dict]):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not self.use_cache:
            return
            
        try:
            with gzip.open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except:
            pass  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¤±æ•—ã¯ç„¡è¦–
    
    def find_project_jsonl_files(self) -> Dict[str, List[Path]]:
        """agent_and_pane_id_table.jsonlã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’èª­ã¿å–ã‚Šã€å¯¾å¿œã™ã‚‹JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        agent_table_path = self.project_root / "Agent-shared" / "agent_and_pane_id_table.jsonl"
        
        if not agent_table_path.exists():
            print(f"âš ï¸  Agent table not found: {agent_table_path}")
            return {}
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        agent_info = {}
        with open(agent_table_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    try:
                        data = json.loads(line)
                        if 'agent_id' in data and 'claude_session_id' in data:
                            agent_info[data['agent_id']] = {
                                'session_id': data['claude_session_id'],
                                'working_dir': data.get('working_dir', ''),
                                'cwd': data.get('cwd', '')  # äº’æ›æ€§ã®ãŸã‚cwdã‚‚ç¢ºèª
                            }
                    except json.JSONDecodeError:
                        continue
        
        if not agent_info:
            print("âš ï¸  No agent sessions found in agent_and_pane_id_table.jsonl")
            return {}
        
        print(f"ğŸ“Š Found {len(agent_info)} agents with session IDs")
        
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¤å®šã¨ãƒ‘ã‚¹å¤‰æ›
        system = platform.system()
        is_wsl = system == "Linux" and "microsoft" in platform.uname().release.lower()
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        agent_files = {}
        for agent_id, info in agent_info.items():
            if not info['session_id']:
                continue
            
            # working_dirã¾ãŸã¯cwdã«åŸºã¥ã„ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®š
            working_dir = info['working_dir'] or info['cwd']
            
            if working_dir:
                # working_dirãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
                full_path = self.project_root / working_dir
            else:
                # PMç­‰working_dirãŒç©ºã®å ´åˆ
                full_path = self.project_root
            
            # ãƒ‘ã‚¹ã‚’Claude projectsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã«å¤‰æ›
            # Claude Codeã®å¤‰æ›ãƒ«ãƒ¼ãƒ«ï¼ˆå®Ÿé¨“ã«ã‚ˆã‚Šåˆ¤æ˜ï¼‰:
            # - è‹±æ•°å­—(a-zA-Z0-9)ä»¥å¤–ã®ã™ã¹ã¦ã®æ–‡å­—ã‚’'-'ã«ç½®æ›
            # - ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã‚‚'-'ã«å¤‰æ›ã•ã‚Œã‚‹
            # ä¾‹: /mnt/c/Users/test_v1.0.0 -> -mnt-c-Users-test-v1-0-0
            import re
            
            # ã¾ãšãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã‚’çµ±ä¸€
            if system == "Windows":
                # Windows: ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«çµ±ä¸€
                path_str = str(full_path).replace('\\', '/')
            else:
                path_str = str(full_path)
            
            # è‹±æ•°å­—ä»¥å¤–ã®ã™ã¹ã¦ã®æ–‡å­—ï¼ˆãƒ‘ã‚¹åŒºåˆ‡ã‚Šã€ç‰¹æ®Šæ–‡å­—ã€ç©ºç™½ç­‰ï¼‰ã‚’'-'ã«ç½®æ›
            dir_name = re.sub(r'[^a-zA-Z0-9]', '-', path_str)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
            project_dir = self.claude_projects_dir / dir_name
            if project_dir.exists():
                jsonl_file = project_dir / f"{info['session_id']}.jsonl"
                if jsonl_file.exists():
                    if agent_id not in agent_files:
                        agent_files[agent_id] = []
                    agent_files[agent_id].append(jsonl_file)
                    print(f"  âœ… {agent_id}: Found log ({jsonl_file.stat().st_size / 1024:.1f} KB)")
                else:
                    print(f"  âš ï¸  {agent_id}: Session file not found: {jsonl_file.name}")
            else:
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€è¿‘ã„åå‰ã‚’æ¢ã™
                similar_dirs = [d for d in self.claude_projects_dir.iterdir() 
                               if d.is_dir() and dir_name.lower() in d.name.lower()]
                if similar_dirs:
                    print(f"  âš ï¸  {agent_id}: Directory not found. Similar: {[d.name for d in similar_dirs[:3]]}")
                else:
                    print(f"  âš ï¸  {agent_id}: Project dir not found: {dir_name}")
        
        return agent_files
    
    def parse_usage_data(self, jsonl_file: Path, agent_id: str, last_n: Optional[int] = None,
                        max_minutes: Optional[int] = None) -> List[Dict]:
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰usageæƒ…å ±ã‚’æŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œã€æ™‚é–“åˆ¶é™å¯¾å¿œï¼‰"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_path = self.get_cache_path(agent_id, jsonl_file)
        cached_data = self.load_from_cache(cache_path, jsonl_file)
        if cached_data is not None:
            # æ™‚é–“åˆ¶é™ã¨last_nã‚’é©ç”¨
            filtered_data = self._apply_time_filter(cached_data, max_minutes)
            if last_n and len(filtered_data) > last_n:
                return filtered_data[-last_n:]
            return filtered_data
        
        # é€šå¸¸ã®è§£æå‡¦ç†
        all_entries = []
        with open(jsonl_file, 'r') as f:
            for line in f:
                if line.strip():
                    try:
                        entry = json.loads(line)
                        # usageãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤ã‚¨ãƒ³ãƒˆãƒªã®ã¿
                        if 'message' in entry and isinstance(entry['message'], dict):
                            msg = entry['message']
                            if 'usage' in msg and isinstance(msg['usage'], dict) and 'timestamp' in entry:
                                all_entries.append({
                                    'timestamp': entry['timestamp'],
                                    'usage': msg['usage']
                                })
                    except (json.JSONDecodeError, KeyError, TypeError):
                        continue
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.save_to_cache(cache_path, all_entries)
        
        # æ™‚é–“åˆ¶é™ã¨last_nã‚’é©ç”¨
        filtered_entries = self._apply_time_filter(all_entries, max_minutes)
        if last_n and len(filtered_entries) > last_n:
            return filtered_entries[-last_n:]
        return filtered_entries
    
    def _apply_time_filter(self, entries: List[Dict], max_minutes: Optional[int]) -> List[Dict]:
        """æ™‚é–“åˆ¶é™ã‚’é©ç”¨ã—ã¦ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if not max_minutes or not entries:
            return entries
        
        # æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        first_timestamp = None
        for entry in entries:
            try:
                ts = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                if first_timestamp is None or ts < first_timestamp:
                    first_timestamp = ts
            except:
                continue
        
        if not first_timestamp:
            return entries
        
        # æ™‚é–“åˆ¶é™ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered = []
        for entry in entries:
            try:
                ts = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                elapsed_minutes = (ts - first_timestamp).total_seconds() / 60
                if elapsed_minutes <= max_minutes:
                    filtered.append(entry)
            except:
                continue
        
        return filtered
    
    def calculate_cumulative_tokens(self, usage_entries: List[Dict], cumulative: bool = False) -> List[Tuple[datetime, Dict[str, int]]]:
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ï¼ˆç´¯ç©ã¾ãŸã¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰"""
        token_data = []
        total_input = 0
        total_cache_creation = 0
        total_cache_read = 0
        total_output = 0
        
        for entry in usage_entries:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¤‰æ›
            ts = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
            
            usage = entry['usage']
            
            if cumulative:
                # ç´¯ç©ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã®å‹•ä½œï¼‰
                total_input += usage.get('input_tokens', 0)
                total_cache_creation += usage.get('cache_creation_input_tokens', 0)
                total_cache_read += usage.get('cache_read_input_tokens', 0)
                total_output += usage.get('output_tokens', 0)
                
                token_data.append((ts, {
                    'input': total_input,
                    'cache_creation': total_cache_creation,
                    'cache_read': total_cache_read,
                    'output': total_output,
                    'total': total_input + total_cache_creation + total_cache_read + total_output
                }))
            else:
                # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå„æ™‚ç‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡ï¼‰
                input_tokens = usage.get('input_tokens', 0)
                cache_creation = usage.get('cache_creation_input_tokens', 0)
                cache_read = usage.get('cache_read_input_tokens', 0)
                output = usage.get('output_tokens', 0)
                
                token_data.append((ts, {
                    'input': input_tokens,
                    'cache_creation': cache_creation,
                    'cache_read': cache_read,
                    'output': output,
                    'total': input_tokens + cache_creation + cache_read + output
                }))
            
        return token_data
    
    def generate_all_graphs(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]],
                           graph_type: str = 'all', time_unit: str = 'minutes', cumulative: bool = False):
        """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        self.is_cumulative = cumulative
        
        if graph_type in ['all', 'overview']:
            self.generate_overview_line_graph(all_agent_data, time_unit)
            
        if graph_type in ['all', 'stacked']:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚«ã‚¦ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ˜è¨˜ãŒã‚ã‚‹ãƒ­ã‚°ï¼‰
            self.generate_stacked_bar_chart(all_agent_data, x_axis='count')
            self.generate_stacked_bar_chart(all_agent_data, x_axis='time')
            
        if graph_type in ['all', 'timeline']:
            self.generate_timeline_graph(all_agent_data)
            
        if graph_type in ['all', 'individual']:
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å€‹åˆ¥ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
            for agent_id, cumulative_data in all_agent_data.items():
                if cumulative_data:
                    self.generate_agent_detail_graphs(agent_id, cumulative_data)
    
    def generate_overview_line_graph(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]], 
                                    time_unit: str = 'minutes'):
        """æ¦‚è¦ç”¨ã®è»½é‡ãªæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        
        Args:
            time_unit: 'seconds', 'minutes', 'hours' ã®ã„ãšã‚Œã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'minutes'ï¼‰
        """
        # åˆ‡ã‚Šã®è‰¯ã„æ™‚é–“ã§è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        milestone_minutes = [30, 60, 90, 120, 180]
        
        # æŒ‡å®šã•ã‚ŒãŸæ™‚é–“åˆ¶é™ã¾ãŸã¯ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã§ç”Ÿæˆ
        if self.max_minutes and self.max_minutes in milestone_minutes:
            # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³æ™‚é–“ã®å ´åˆã€ãã®æ™‚é–“ã¾ã§ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
            self._generate_single_overview_graph(all_agent_data, time_unit, self.max_minutes)
        elif self.max_minutes:
            # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ä»¥å¤–ã®æ™‚é–“æŒ‡å®š
            self._generate_single_overview_graph(all_agent_data, time_unit, self.max_minutes)
        else:
            # æ™‚é–“æŒ‡å®šãªã—ã®å ´åˆã€å…¨ä½“ã¨ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            # å…¨ä½“ã‚°ãƒ©ãƒ•
            self._generate_single_overview_graph(all_agent_data, time_unit, None)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’ç¢ºèª
            project_start = self._get_project_start_time(all_agent_data)
            if project_start:
                # ç¾åœ¨ã¾ã§ã®çµŒéæ™‚é–“ã‚’è¨ˆç®—
                latest_time = max([t for data in all_agent_data.values() for t, _ in data]) if all_agent_data else None
                if latest_time:
                    elapsed_minutes = (latest_time - project_start).total_seconds() / 60
                    
                    # çµŒéæ™‚é–“ã‚’è¶…ãˆãŸãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã®ã¿ç”Ÿæˆ
                    for milestone in milestone_minutes:
                        if elapsed_minutes >= milestone:
                            self._generate_single_overview_graph(all_agent_data, time_unit, milestone)
    
    def _generate_single_overview_graph(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]], 
                                       time_unit: str, max_minutes: Optional[int]):
        """å˜ä¸€ã®æ¦‚è¦ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        plt.figure(figsize=(12, 8))
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã«æ™‚é–“åˆ¶é™ã‚’è¡¨ç¤º
        title_suffix = f" (First {max_minutes} minutes)" if max_minutes else ""
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹æ™‚åˆ»ã‚’å–å¾—
        project_start = self._get_project_start_time(all_agent_data)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹æ™‚åˆ»ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_agent_data = {}
        if project_start:
            for agent_id, cumulative_data in all_agent_data.items():
                # max_minutesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã®ç¯„å›²å†…ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
                if max_minutes:
                    filtered_data = [(t, tokens) for t, tokens in cumulative_data 
                                   if t >= project_start and 
                                   (t - project_start).total_seconds() / 60 <= max_minutes]
                else:
                    filtered_data = [(t, tokens) for t, tokens in cumulative_data if t >= project_start]
                if filtered_data:
                    filtered_agent_data[agent_id] = filtered_data
        else:
            filtered_agent_data = all_agent_data
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¨ç§»
        for agent_id, cumulative_data in filtered_agent_data.items():
            if not cumulative_data:
                continue
                
            # ç›¸å¯¾æ™‚é–“ã«å¤‰æ›ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆ†å˜ä½ï¼‰
            time_divisor = {'seconds': 1, 'minutes': 60, 'hours': 3600}[time_unit]
            times = [(t - project_start).total_seconds() / time_divisor for t, _ in cumulative_data]
            totals = [tokens['total'] for _, tokens in cumulative_data]
            
            # ã‚¹ãƒ†ãƒƒãƒ—ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆéšæ®µçŠ¶ï¼‰ã®æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
            plt.step(times, totals, where='post', marker='o', markersize=3, 
                    label=agent_id, alpha=0.8)
        
        # é–¾å€¤ãƒ©ã‚¤ãƒ³
        plt.axhline(y=self.AUTO_COMPACT_THRESHOLD, color='red', 
                   linestyle='--', linewidth=2, label='Auto-compact (~160K)')
        plt.axhline(y=self.WARNING_THRESHOLD, color='orange', 
                   linestyle='--', linewidth=1, label='Warning (140K)')
        
        # Xè»¸ãƒ©ãƒ™ãƒ«ï¼ˆå˜ä½ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
        unit_labels = {'seconds': 'Seconds', 'minutes': 'Minutes', 'hours': 'Hours'}
        plt.xlabel(f'{unit_labels[time_unit]} from Project Start')
        
        # Yè»¸ãƒ©ãƒ™ãƒ«ï¼ˆç´¯ç©ãƒ¢ãƒ¼ãƒ‰ã§å¤‰æ›´ï¼‰
        if hasattr(self, 'is_cumulative') and self.is_cumulative:
            plt.ylabel('Cumulative Token Usage')
            plt.title(f'Cumulative Token Usage Over Time{title_suffix}')
        else:
            plt.ylabel('Current Context Usage [tokens]')
            plt.title(f'Context Usage Monitor{title_suffix}')
        plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
        plt.grid(True, alpha=0.3)
        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))
        
        # Xè»¸ã®ç¯„å›²ã‚’æ™‚é–“åˆ¶é™ã«åˆã‚ã›ã¦è¨­å®š
        if max_minutes:
            plt.xlim(0, max_minutes)
        
        plt.tight_layout()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã«æ™‚é–“åˆ¶é™ã‚’å«ã‚ã‚‹
        if max_minutes:
            # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³æ™‚é–“ã®å ´åˆã€ç‰¹åˆ¥ãªãƒ•ã‚¡ã‚¤ãƒ«å
            if max_minutes in [30, 60, 90, 120, 180]:
                output_path = self.output_dir / f"context_usage_{max_minutes}min.png"
            else:
                output_path = self.output_dir / f"context_usage_overview_{max_minutes}min.png"
        else:
            output_path = self.output_dir / "context_usage_overview.png"
        
        plt.savefig(output_path, dpi=120, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ¦‚è¦ã‚°ãƒ©ãƒ•ç”Ÿæˆ: {output_path}")
    
    def _get_project_start_time(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]]) -> Optional[datetime]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹æ™‚åˆ»ã‚’å–å¾—"""
        start_time_file = self.project_root / "Agent-shared" / "project_start_time.txt"
        project_start = None
        
        if start_time_file.exists():
            try:
                with open(start_time_file, 'r') as f:
                    time_str = f.read().strip()
                    project_start = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
            except:
                pass
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã®æœ€ã‚‚å¤ã„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½¿ç”¨
        if project_start is None:
            for agent_data in all_agent_data.values():
                if agent_data and (project_start is None or agent_data[0][0] < project_start):
                    project_start = agent_data[0][0]
        
        return project_start
    
    def generate_stacked_bar_chart(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]],
                                  x_axis: str = 'count'):
        """ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ï¼ˆé™çš„ãªã‚‚ã®ã‚’ä¸‹ã«é…ç½®ï¼‰"""
        fig, ax = plt.subplots(figsize=(16, 10))
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ï¼ˆé™çš„â†’å‹•çš„ã®é †ï¼‰
        token_types = ['cache_read', 'cache_creation', 'input', 'output']
        token_colors = {
            'cache_read': '#f39c12',     # ã‚ªãƒ¬ãƒ³ã‚¸ï¼ˆæœ€ã‚‚é™çš„ï¼‰
            'cache_creation': '#2ecc71',  # ç·‘
            'input': '#3498db',          # é’
            'output': '#e74c3c'          # èµ¤ï¼ˆæœ€ã‚‚å‹•çš„ï¼‰
        }
        
        if x_axis == 'count':
            # ãƒ­ã‚°å›æ•°ãƒ™ãƒ¼ã‚¹ã®æ£’ã‚°ãƒ©ãƒ•
            bar_width = 0.8
            agent_positions = {}
            
            for idx, (agent_id, cumulative_data) in enumerate(all_agent_data.items()):
                if not cumulative_data:
                    continue
                    
                # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
                latest_time, latest_tokens = cumulative_data[-1]
                
                # Xè»¸ã®ä½ç½®
                x_pos = idx
                agent_positions[agent_id] = x_pos
                
                # ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ï¼ˆé™çš„ãªã‚‚ã®ã‹ã‚‰ï¼‰
                bottom = 0
                for token_type in token_types:
                    value = latest_tokens[token_type]
                    ax.bar(x_pos, value, bar_width, bottom=bottom,
                          color=token_colors[token_type], 
                          label=token_type if idx == 0 else "")
                    bottom += value
                
                # åˆè¨ˆå€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
                total = latest_tokens['total']
                percentage = (total / self.AUTO_COMPACT_THRESHOLD) * 100
                ax.text(x_pos, total + 2000, f'{total:,}\n({percentage:.1f}%)', 
                       ha='center', va='bottom', fontsize=9)
            
            ax.set_xticks(list(agent_positions.values()))
            ax.set_xticklabels(list(agent_positions.keys()))
            ax.set_xlabel('Agents')
            
        else:  # x_axis == 'time'
            # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ç©ã¿ä¸Šã’é¢ã‚°ãƒ©ãƒ•
            # æœ€ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            max_agent = max(all_agent_data.items(), 
                          key=lambda x: x[1][-1][1]['total'] if x[1] else 0)[0]
            
            if all_agent_data[max_agent]:
                data = all_agent_data[max_agent]
                times = [t for t, _ in data]
                
                # å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ã®å€¤ã‚’å–å¾—
                token_values = {tt: [tokens[tt] for _, tokens in data] for tt in token_types}
                
                # ç©ã¿ä¸Šã’é¢ã‚°ãƒ©ãƒ•
                bottom = np.zeros(len(times))
                for token_type in token_types:
                    values = np.array(token_values[token_type])
                    ax.fill_between(times, bottom, bottom + values, 
                                   color=token_colors[token_type],
                                   label=token_type, alpha=0.8)
                    bottom += values
                
                ax.set_xlabel('Time')
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
                plt.xticks(rotation=45)
                ax.set_title(f'Token Usage Timeline - {max_agent}')
        
        # å…±é€šè¨­å®š
        ax.axhline(y=self.AUTO_COMPACT_THRESHOLD, color='red', 
                  linestyle='--', linewidth=2, label='Auto-compact (~160K)')
        ax.axhline(y=self.WARNING_THRESHOLD, color='orange', 
                  linestyle='--', linewidth=1, label='Warning (140K)')
        
        ax.set_ylabel('Cumulative Tokens')
        ax.set_title(f'VibeCodeHPC Token Usage (X-axis: {x_axis})')
        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim(0, self.CONTEXT_LIMIT * 1.05)
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))
        
        plt.tight_layout()
        output_path = self.output_dir / f"context_usage_stacked_{x_axis}.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ç©ã¿ä¸Šã’ã‚°ãƒ©ãƒ•ç”Ÿæˆï¼ˆ{x_axis}è»¸ï¼‰: {output_path}")
    
    def generate_timeline_graph(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]]):
        """auto-compactäºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚°ãƒ©ãƒ•"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), 
                                       gridspec_kw={'height_ratios': [2, 1]})
        
        # ä¸Šæ®µ: å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¨ç§»
        for agent_id, cumulative_data in all_agent_data.items():
            if not cumulative_data:
                continue
                
            times = [t for t, _ in cumulative_data]
            totals = [tokens['total'] for _, tokens in cumulative_data]
            
            # ç¾åœ¨ã®ä½¿ç”¨ç‡ã«å¿œã˜ã¦è‰²ã‚’å¤‰ãˆã‚‹
            current_usage = totals[-1] if totals else 0
            if current_usage >= self.AUTO_COMPACT_THRESHOLD * 0.95:
                color = 'red'
                alpha = 1.0
            elif current_usage >= self.WARNING_THRESHOLD:
                color = 'orange'
                alpha = 0.8
            else:
                color = 'blue'
                alpha = 0.6
                
            ax1.step(times, totals, where='post', marker='o', markersize=3, 
                    label=f'{agent_id} ({current_usage/1000:.0f}K)', 
                    color=color, alpha=alpha)
        
        ax1.axhline(y=self.AUTO_COMPACT_THRESHOLD, color='red', 
                   linestyle='--', linewidth=2)
        ax1.set_ylabel('Total Tokens')
        ax1.set_title('Context Usage Timeline & Auto-compact Prediction')
        ax1.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
        ax1.grid(True, alpha=0.3)
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))
        
        # ä¸‹æ®µ: å¢—åŠ ç‡ã®å¯è¦–åŒ–
        self._plot_growth_rates(ax2, all_agent_data)
        
        plt.tight_layout()
        output_path = self.output_dir / "context_usage_timeline.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ç”Ÿæˆ: {output_path}")
    
    def generate_agent_detail_graphs(self, agent_id: str, cumulative_data: List[Tuple[datetime, Dict[str, int]]]):
        """å€‹åˆ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©³ç´°ã‚°ãƒ©ãƒ•ï¼ˆ2ç¨®é¡ï¼‰"""
        
        # 1. æ™‚ç³»åˆ—ç©ã¿ä¸Šã’é¢ã‚°ãƒ©ãƒ•
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), 
                                       gridspec_kw={'height_ratios': [2, 1]})
        
        times = [t for t, _ in cumulative_data]
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ï¼ˆé™çš„â†’å‹•çš„ã®é †ï¼‰
        token_types = ['cache_read', 'cache_creation', 'input', 'output']
        token_colors = {
            'cache_read': '#f39c12',     # ã‚ªãƒ¬ãƒ³ã‚¸
            'cache_creation': '#2ecc71',  # ç·‘
            'input': '#3498db',          # é’
            'output': '#e74c3c'          # èµ¤
        }
        
        # ä¸Šæ®µ: ç©ã¿ä¸Šã’é¢ã‚°ãƒ©ãƒ•
        token_values = {tt: [tokens[tt] for _, tokens in cumulative_data] for tt in token_types}
        bottom = np.zeros(len(times))
        
        for token_type in token_types:
            values = np.array(token_values[token_type])
            ax1.fill_between(times, bottom, bottom + values, 
                           color=token_colors[token_type],
                           label=token_type, alpha=0.8)
            bottom += values
        
        # æœ€æ–°ã®çµ±è¨ˆæƒ…å ±
        latest_tokens = cumulative_data[-1][1]
        total = latest_tokens['total']
        percentage = (total / self.AUTO_COMPACT_THRESHOLD) * 100
        
        # é–¾å€¤ãƒ©ã‚¤ãƒ³
        ax1.axhline(y=self.AUTO_COMPACT_THRESHOLD, color='red', 
                   linestyle='--', linewidth=2, label='Auto-compact')
        ax1.axhline(y=self.WARNING_THRESHOLD, color='orange', 
                   linestyle='--', linewidth=1, label='Warning')
        
        ax1.set_ylabel('Cumulative Tokens')
        ax1.set_title(f'{agent_id} - Token Usage Detail ({total:,} tokens, {percentage:.1f}%)')
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
        
        # ä¸‹æ®µ: å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ã®å‰²åˆæ¨ç§»
        self._plot_token_ratios(ax2, cumulative_data, token_types)
        
        plt.tight_layout()
        output_path = self.output_dir / f"context_usage_{agent_id}_detail.png"
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # 2. ãƒ­ã‚°å›æ•°ãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ©ãƒ•
        self._generate_count_based_graph(agent_id, cumulative_data)
        
        print(f"âœ… {agent_id} ã®å€‹åˆ¥ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†")
    
    def _plot_token_ratios(self, ax, cumulative_data: List[Tuple[datetime, Dict[str, int]]], 
                          token_types: List[str]):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ã®å‰²åˆæ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        times = [t for t, _ in cumulative_data]
        
        # å„æ™‚ç‚¹ã§ã®å‰²åˆã‚’è¨ˆç®—
        ratios = {tt: [] for tt in token_types}
        
        for _, tokens in cumulative_data:
            total = tokens['total']
            if total > 0:
                for tt in token_types:
                    ratios[tt].append(100 * tokens[tt] / total)
            else:
                for tt in token_types:
                    ratios[tt].append(0)
        
        # å‰²åˆã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        for token_type in token_types:
            ax.plot(times, ratios[token_type], marker='o', markersize=3, 
                   label=f'{token_type} %', alpha=0.7)
        
        ax.set_xlabel('Time')
        ax.set_ylabel('Token Type Ratio (%)')
        ax.set_title('Token Type Distribution Over Time')
        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
        ax.grid(True, alpha=0.3)
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        ax.set_ylim(0, 100)
    
    def _generate_count_based_graph(self, agent_id: str, cumulative_data: List[Tuple[datetime, Dict[str, int]]]):
        """ãƒ­ã‚°å›æ•°ãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ©ãƒ•"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Xè»¸: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªç•ªå·
        log_counts = list(range(1, len(cumulative_data) + 1))
        totals = [tokens['total'] for _, tokens in cumulative_data]
        
        # è‰²åˆ†ã‘ï¼ˆä½¿ç”¨ç‡ã«å¿œã˜ã¦ï¼‰
        colors = []
        for total in totals:
            if total >= self.AUTO_COMPACT_THRESHOLD * 0.95:
                colors.append('red')
            elif total >= self.WARNING_THRESHOLD:
                colors.append('orange')
            else:
                colors.append('blue')
        
        # æ•£å¸ƒå›³ã¨ç·š
        ax.scatter(log_counts, totals, c=colors, s=50, alpha=0.7, edgecolors='black')
        ax.plot(log_counts, totals, 'b-', alpha=0.3)
        
        # é–¾å€¤ãƒ©ã‚¤ãƒ³
        ax.axhline(y=self.AUTO_COMPACT_THRESHOLD, color='red', 
                  linestyle='--', linewidth=2, label='Auto-compact')
        ax.axhline(y=self.WARNING_THRESHOLD, color='orange', 
                  linestyle='--', linewidth=1, label='Warning')
        
        ax.set_xlabel('Log Entry Count')
        ax.set_ylabel('Total Tokens')
        ax.set_title(f'{agent_id} - Token Usage by Log Count')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K'))
        
        plt.tight_layout()
        output_path = self.output_dir / f"context_usage_{agent_id}_count.png"
        plt.savefig(output_path, dpi=120, bbox_inches='tight')
        plt.close()
    
    def _plot_growth_rates(self, ax, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]]):
        """ãƒˆãƒ¼ã‚¯ãƒ³å¢—åŠ ç‡ã‚’å¯è¦–åŒ–"""
        
        for agent_id, cumulative_data in all_agent_data.items():
            if len(cumulative_data) < 2:
                continue
                
            times = [t for t, _ in cumulative_data]
            totals = [tokens['total'] for _, tokens in cumulative_data]
            
            # å¢—åŠ ç‡ã‚’è¨ˆç®—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³/æ™‚é–“ï¼‰
            growth_rates = []
            growth_times = []
            
            for i in range(1, len(times)):
                time_diff = (times[i] - times[i-1]).total_seconds() / 3600  # æ™‚é–“å˜ä½
                if time_diff > 0:
                    token_diff = totals[i] - totals[i-1]
                    rate = token_diff / time_diff
                    growth_rates.append(rate)
                    growth_times.append(times[i])
            
            if growth_rates:
                ax.plot(growth_times, growth_rates, marker='o', markersize=3, 
                       label=agent_id, alpha=0.7)
        
        ax.set_xlabel('Time')
        ax.set_ylabel('Growth Rate (tokens/hour)')
        ax.set_title('Token Growth Rate Analysis')
        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
        ax.grid(True, alpha=0.3)
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
    
    def generate_summary_report(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]]):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
        report_path = self.output_dir / "context_usage_report.md"
        
        with open(report_path, 'w') as f:
            # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆç´¯ç©ãƒ¢ãƒ¼ãƒ‰ã§å¤‰æ›´ï¼‰
            if hasattr(self, 'is_cumulative') and self.is_cumulative:
                f.write("# ç´¯ç©ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            else:
                f.write("# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ã‚µãƒãƒªãƒ¼\n\n")
            f.write("| ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | åˆè¨ˆ [ãƒˆãƒ¼ã‚¯ãƒ³] | ä½¿ç”¨ç‡ | Cache Read | Cache Create | Input | Output | æ¨å®šæ™‚é–“ |\n")
            f.write("|-------------|----------------|--------|------------|--------------|-------|--------|----------|\n")
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
            agent_summaries = []
            
            for agent_id, cumulative_data in all_agent_data.items():
                if not cumulative_data:
                    continue
                    
                latest_time, latest_tokens = cumulative_data[-1]
                total = latest_tokens['total']
                percentage = (total / self.AUTO_COMPACT_THRESHOLD) * 100
                
                # auto-compactã¾ã§ã®æ¨å®šæ™‚é–“
                est_hours = "N/A"
                if len(cumulative_data) >= 2:
                    # ç›´è¿‘ã®å¢—åŠ ç‡ã‹ã‚‰æ¨å®š
                    recent_data = cumulative_data[-min(10, len(cumulative_data)):]
                    time_span = (recent_data[-1][0] - recent_data[0][0]).total_seconds() / 3600
                    token_increase = recent_data[-1][1]['total'] - recent_data[0][1]['total']
                    
                    if time_span > 0 and token_increase > 0:
                        rate = token_increase / time_span
                        remaining_tokens = self.AUTO_COMPACT_THRESHOLD - total
                        if remaining_tokens > 0:
                            est_hours = f"{remaining_tokens / rate:.1f}h"
                
                # çŠ¶æ…‹ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆç´¯ç©ãƒ¢ãƒ¼ãƒ‰ã§ã¯å¸¸ã«ç·‘ï¼‰
                if hasattr(self, 'is_cumulative') and self.is_cumulative:
                    status = "ğŸŸ¢"  # ç´¯ç©ãƒ¢ãƒ¼ãƒ‰ã§ã¯é–¾å€¤åˆ¤å®šãªã—
                else:
                    if total >= self.AUTO_COMPACT_THRESHOLD * 0.95:
                        status = "ğŸ”´"
                    elif total >= self.WARNING_THRESHOLD:
                        status = "ğŸŸ¡"
                    else:
                        status = "ğŸŸ¢"
                
                agent_summaries.append({
                    'agent_id': agent_id,
                    'status': status,
                    'total': total,
                    'percentage': percentage,
                    'tokens': latest_tokens,
                    'est_hours': est_hours
                })
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
            agent_summaries.sort(key=lambda x: x['total'], reverse=True)
            
            for summary in agent_summaries:
                f.write(f"| {summary['status']} {summary['agent_id']} | "
                       f"{summary['total']:,} | "
                       f"{summary['percentage']:.1f}% | "
                       f"{summary['tokens']['cache_read']:,} | "
                       f"{summary['tokens']['cache_creation']:,} | "
                       f"{summary['tokens']['input']:,} | "
                       f"{summary['tokens']['output']:,} | "
                       f"{summary['est_hours']} |\n")
            
            f.write("\n## Visualizations\n\n")
            f.write("### Global Views\n")
            f.write("- [Overview](context_usage_overview.png) - è»½é‡ãªæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•\n")
            f.write("- [Stacked by Count](context_usage_stacked_count.png) - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç©ã¿ä¸Šã’\n")
            f.write("- [Stacked by Time](context_usage_stacked_time.png) - æ™‚ç³»åˆ—ç©ã¿ä¸Šã’\n")
            f.write("- [Timeline](context_usage_timeline.png) - äºˆæ¸¬ã¨ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ\n\n")
            
            f.write("### Individual Agent Details\n")
            for agent_id in sorted(all_agent_data.keys()):
                f.write(f"- {agent_id}: [Detail](context_usage_{agent_id}_detail.png) | "
                       f"[Count](context_usage_{agent_id}_count.png)\n")
            
            f.write("\n## Quick Access Commands\n\n")
            f.write("```bash\n")
            f.write("# æœ€æ–°çŠ¶æ…‹ã®ç¢ºèªï¼ˆãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ï¼‰\n")
            f.write("python telemetry/context_usage_monitor.py --status\n\n")
            f.write("# ç‰¹å®šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹ç¢ºèª\n")
            f.write("python telemetry/context_usage_monitor.py --status --agent PG1.1.1\n\n")
            f.write("# æ¦‚è¦ã‚°ãƒ©ãƒ•ã®ã¿ç”Ÿæˆï¼ˆè»½é‡ï¼‰\n")
            f.write("python telemetry/context_usage_monitor.py --graph-type overview\n")
            f.write("```\n\n")
            
            f.write("## Cache Status\n\n")
            if self.use_cache and self.cache_dir.exists():
                cache_size = sum(f.stat().st_size for f in self.cache_dir.glob('*.pkl.gz'))
                f.write(f"- Cache directory: `.cache/context_monitor/`\n")
                f.write(f"- Total cache size: {cache_size / 1024 / 1024:.1f} MB\n")
                f.write(f"- Cache files: {len(list(self.cache_dir.glob('*.pkl.gz')))}\n")
            else:
                f.write("- Cache: Disabled\n")
        
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
    
    def print_quick_status(self, all_agent_data: Dict[str, List[Tuple[datetime, Dict[str, int]]]], 
                          target_agent: Optional[str] = None):
        """ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ç¾åœ¨ã®çŠ¶æ…‹ã‚’å‡ºåŠ›ï¼ˆã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰"""
        
        print("\n" + "="*60)
        print(f"VibeCodeHPC Context Usage Status - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if target_agent:
            filtered_data = {k: v for k, v in all_agent_data.items() 
                           if target_agent.upper() in k.upper()}
        else:
            filtered_data = all_agent_data
        
        if not filtered_data:
            print(f"âŒ Agent '{target_agent}' not found")
            return
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§å‡ºåŠ›
        print(f"{'Agent':<10} {'Total':>10} {'%':>6} {'Status':<8} {'Est.Time':<10}")
        print("-"*50)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¦ã‚½ãƒ¼ãƒˆ
        agent_infos = []
        for agent_id, cumulative_data in filtered_data.items():
            if not cumulative_data:
                continue
                
            latest_time, latest_tokens = cumulative_data[-1]
            total = latest_tokens['total']
            percentage = (total / self.AUTO_COMPACT_THRESHOLD) * 100
            
            # çŠ¶æ…‹åˆ¤å®š
            if total >= self.AUTO_COMPACT_THRESHOLD * 0.95:
                status = "ğŸ”´ CRITICAL"
            elif total >= self.WARNING_THRESHOLD:
                status = "ğŸŸ¡ WARNING"
            else:
                status = "ğŸŸ¢ OK"
            
            # æ¨å®šæ™‚é–“
            est_time = "N/A"
            if len(cumulative_data) >= 2:
                recent_data = cumulative_data[-min(10, len(cumulative_data)):]
                time_span = (recent_data[-1][0] - recent_data[0][0]).total_seconds() / 3600
                token_increase = recent_data[-1][1]['total'] - recent_data[0][1]['total']
                
                if time_span > 0 and token_increase > 0:
                    rate = token_increase / time_span
                    remaining_tokens = self.AUTO_COMPACT_THRESHOLD - total
                    if remaining_tokens > 0:
                        hours = remaining_tokens / rate
                        if hours < 1:
                            est_time = f"{int(hours*60)}min"
                        else:
                            est_time = f"{hours:.1f}h"
            
            agent_infos.append({
                'agent_id': agent_id,
                'total': total,
                'percentage': percentage,
                'status': status,
                'est_time': est_time
            })
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
        agent_infos.sort(key=lambda x: x['total'], reverse=True)
        
        # å‡ºåŠ›
        for info in agent_infos:
            print(f"{info['agent_id']:<10} {info['total']:>10,} {info['percentage']:>5.1f}% "
                  f"{info['status']:<8} {info['est_time']:<10}")
        
        print("\n" + "="*60)

def get_python_command():
    """åˆ©ç”¨å¯èƒ½ãªPythonã‚³ãƒãƒ³ãƒ‰ã‚’å–å¾—"""
    commands = ['python3', 'python']
    
    for cmd in commands:
        try:
            result = subprocess.run([cmd, '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                return cmd
        except FileNotFoundError:
            continue
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return 'python3'

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='Monitor Claude Code context usage')
    parser.add_argument('--last-n', type=int, default=None,
                       help='Analyze only the last N log entries per agent')
    parser.add_argument('--graph-type', choices=['all', 'overview', 'stacked', 'timeline', 'individual'],
                       default='all', help='Type of graphs to generate')
    parser.add_argument('--time-unit', choices=['seconds', 'minutes', 'hours'],
                       default='minutes', help='Time unit for X-axis (default: minutes)')
    parser.add_argument('--cumulative', action='store_true',
                       help='Show cumulative token usage instead of per-request context size')
    parser.add_argument('--max-minutes', type=int, default=None,
                       help='Limit graph to first N minutes from project start (e.g., 60, 120, 180)')
    parser.add_argument('--no-cache', action='store_true',
                       help='Disable caching')
    parser.add_argument('--clear-cache', action='store_true',
                       help='Clear cache before running')
    parser.add_argument('--watch', action='store_true', 
                       help='Continue monitoring (update every 5 minutes)')
    parser.add_argument('--interval', type=int, default=300,
                       help='Update interval in seconds (default: 300)')
    parser.add_argument('--status', action='store_true',
                       help='Show quick status in console (no graphs)')
    parser.add_argument('--agent', type=str, default=None,
                       help='Show status for specific agent only')
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’å–å¾—
    project_root = Path(__file__).parent.parent
    monitor = ContextUsageMonitor(project_root, use_cache=not args.no_cache, max_minutes=args.max_minutes)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
    if args.clear_cache and monitor.cache_dir.exists():
        import shutil
        shutil.rmtree(monitor.cache_dir)
        monitor.cache_dir.mkdir(parents=True, exist_ok=True)
        print("âœ… Cache cleared")
    
    def update_once():
        """ä¸€åº¦ã ã‘æ›´æ–°"""
        print("ğŸ” Scanning agent_and_pane_id_table.jsonl for session IDs...")
        jsonl_files = monitor.find_project_jsonl_files()
        
        if not jsonl_files:
            print("âŒ No JSONL files found for agents")
            print(f"   Check: {monitor.project_root / 'Agent-shared' / 'agent_and_pane_id_table.jsonl'}")
            return
        
        print(f"ğŸ“Š Found {len(jsonl_files)} agents with logs")
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        all_agent_data = {}
        for agent_id, files in jsonl_files.items():
            if not args.status:  # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºæ™‚ã¯é€²æ—ã‚’çœç•¥
                print(f"  - Processing {agent_id}...")
            
            # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯çµåˆ
            all_usage_entries = []
            for jsonl_file in sorted(files):
                entries = monitor.parse_usage_data(jsonl_file, agent_id, args.last_n, args.max_minutes)
                all_usage_entries.extend(entries)
            
            if all_usage_entries:
                # æ™‚ç³»åˆ—ã§ã‚½ãƒ¼ãƒˆ
                all_usage_entries.sort(key=lambda x: x['timestamp'])
                cumulative_data = monitor.calculate_cumulative_tokens(all_usage_entries, args.cumulative)
                all_agent_data[agent_id] = cumulative_data
        
        if all_agent_data:
            if args.status:
                # ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
                monitor.print_quick_status(all_agent_data, args.agent)
            else:
                # ã‚°ãƒ©ãƒ•ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                monitor.generate_all_graphs(all_agent_data, args.graph_type, args.time_unit, args.cumulative)
                monitor.generate_summary_report(all_agent_data)
                print("âœ… Context usage monitoring complete")
        else:
            print("âŒ No usage data found in JSONL files")
    
    # å®Ÿè¡Œ
    if args.watch:
        import time
        print(f"ğŸ‘ï¸  Watching mode enabled (interval: {args.interval}s)")
        while True:
            update_once()
            print(f"ğŸ’¤ Waiting {args.interval}s...")
            time.sleep(args.interval)
    else:
        update_once()

if __name__ == "__main__":
    # å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    try:
        import matplotlib
        import numpy
        main()
    except ImportError:
        print("âŒ Error: Required packages not installed")
        print("Please install: pip3 install -r requirements.txt")
        sys.exit(1)